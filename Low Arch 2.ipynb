{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90753323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "1370\n",
      "340\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transform\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "img_transform = transform.Compose([transform.ToTensor(), transform.Normalize((0.5,),(0.5,))]) \n",
    "\n",
    "train_set = torchvision.datasets.ImageFolder('/home/jaoks/Desktop/proyecto6IA/dataset/train', transform=transform.ToTensor())\n",
    "val_set = torchvision.datasets.ImageFolder('/home/jaoks/Desktop/proyecto6IA/dataset/val', transform=transform.ToTensor())\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5b50d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_resol_train = torch.utils.data.Subset(train_set, range(685))\n",
    "low_resol_train = torch.utils.data.Subset(train_set, range(685, 1370))\n",
    "\n",
    "hd_resol_val = torch.utils.data.Subset(val_set, range(0, 119))\n",
    "low_resol_val = torch.utils.data.Subset(val_set, range(170, 289))\n",
    "\n",
    "hd_resol_test = torch.utils.data.Subset(val_set, range(119, 170))\n",
    "low_resol_test = torch.utils.data.Subset(val_set, range(289, 340))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4543b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_low = torch.utils.data.DataLoader(dataset=low_resol_train, batch_size=batch_size, shuffle=True)\n",
    "train_loader_hd = torch.utils.data.DataLoader(dataset=hd_resol_train, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "val_loader_low =  torch.utils.data.DataLoader(dataset=low_resol_val, batch_size=batch_size, shuffle=True)\n",
    "val_loader_hd =  torch.utils.data.DataLoader(dataset=hd_resol_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader_low =  torch.utils.data.DataLoader(dataset=low_resol_test, batch_size=17, shuffle=True)\n",
    "test_loader_hd =  torch.utils.data.DataLoader(dataset=hd_resol_test, batch_size=17, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "efd6c22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 6, 64, 64])\n",
      "torch.Size([1, 12, 32, 32])\n",
      "torch.Size([1, 128, 11, 11])\n",
      "LAST OUT 4\n",
      "torch.Size([1, 256, 3, 3])\n",
      "tensor([[-0.0048]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 256, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=2, stride=2, padding=0)\n",
    "bn1 = nn.BatchNorm2d(6)\n",
    "pool = nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=2, stride=2, padding=0)\n",
    "conv3 = nn.Conv2d(in_channels=12, out_channels=128, kernel_size=2, stride=3, padding=1)\n",
    "conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=4, padding=0)\n",
    "tempimg, _ = train_set[0]\n",
    "#print(tempimg.shape)\n",
    "tempimg = tempimg.unsqueeze(0)\n",
    "print(tempimg.shape)\n",
    "out_1 = pool(F.relu(bn1(conv1(tempimg))))\n",
    "print(out_1.shape)\n",
    "out1 = F.relu(conv2(out_1))\n",
    "print(out1.shape)\n",
    "out_3 = F.relu(conv3(out1))\n",
    "print(out_3.shape)\n",
    "out_4 = F.relu(conv4(out_3))\n",
    "print(\"LAST OUT 4\")\n",
    "print(out_4.shape)\n",
    "out1 = out_4.view(out1.size(0), -1)\n",
    "fc = nn.Linear(in_features=256*3*3, out_features=1)\n",
    "z = fc(out1)\n",
    "print(z)\n",
    "\n",
    "fcT = nn.Linear(in_features=1, out_features=256*3*3)\n",
    "out1 = fcT(z)\n",
    "#print(out1.shape)\n",
    "out1 = out1.view(1, 256, 3, 3)\n",
    "print(out1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ecf6a6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE DECO\n",
      "torch.Size([1, 256, 3, 3])\n",
      "OUTPUT CT1\n",
      "torch.Size([1, 128, 9, 9])\n",
      "OUTPUT CT2\n",
      "torch.Size([1, 12, 23, 23])\n",
      "OUTPUT CT3\n",
      "torch.Size([1, 6, 64, 64])\n",
      "torch.Size([1, 12, 64, 64])\n",
      "OUTPUT CT4\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "convTran1 = nn.ConvTranspose2d(in_channels=256,out_channels=128, kernel_size=3, stride=3, padding=0)\n",
    "convTran2 = nn.ConvTranspose2d(in_channels=128,out_channels=12, kernel_size=3, stride=3, padding=2)\n",
    "convTran3 = nn.ConvTranspose2d(in_channels=12,out_channels=6, kernel_size=4, stride=3, padding=3)\n",
    "convTran4 = nn.ConvTranspose2d(in_channels=12,out_channels=3, kernel_size=4, stride=4, padding=0 )\n",
    "\n",
    "out1 = F.relu(convTran1(out1))\n",
    "print(\"OUTPUT CT1\")\n",
    "print(out1.shape)\n",
    "\n",
    "out1 = F.relu(convTran2(out1))\n",
    "print(\"OUTPUT CT2\")\n",
    "print(out1.shape)\n",
    "\n",
    "out1 = F.relu(convTran3(out1))\n",
    "print(\"OUTPUT CT3\")\n",
    "print(out1.shape)\n",
    "\n",
    "out1 = torch.cat([out1, out_1], 1)\n",
    "print(out1.shape)\n",
    "out1 = F.relu(convTran4(out1))\n",
    "print(\"OUTPUT CT4\")\n",
    "print(out1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "63ff9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=2, stride=2, padding=0)\n",
    "    self.bn1 = nn.BatchNorm2d(6)\n",
    "    self.pool = nn.MaxPool2d(kernel_size = 2, stride=2)\n",
    "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=2, stride=2, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=12, out_channels=128, kernel_size=2, stride=3, padding=1)\n",
    "    self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=4, padding=0)\n",
    "    self.fc = nn.Linear(in_features=256*3*3, out_features=1)\n",
    "\n",
    "  def forward(self, image):\n",
    "    out_1 = self.pool(F.tanh(self.bn1(self.conv1(image))))\n",
    "    out = F.tanh(self.conv2(out_1))\n",
    "    out_3 = F.tanh(self.conv3(out))\n",
    "    out_4 = F.tanh(self.conv4(out_3))\n",
    "    out = out_4.view(out_4.size(0), -1)\n",
    "    z = self.fc(out)\n",
    "    return z, out_1, out_3, out_4\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.fc    = nn.Linear(in_features=1,out_features=256*3*3)\n",
    "    self.convTran1 = nn.ConvTranspose2d(in_channels=256,out_channels=128, kernel_size=3, stride=3, padding=0)\n",
    "    self.convTran2 = nn.ConvTranspose2d(in_channels=128,out_channels=12, kernel_size=3, stride=3, padding=2)\n",
    "    self.convTran3 = nn.ConvTranspose2d(in_channels=12,out_channels=6, kernel_size=4, stride=3, padding=3)\n",
    "    self.convTran4 = nn.ConvTranspose2d(in_channels=12,out_channels=3, kernel_size=4, stride=4, padding=0)\n",
    "    \n",
    "  def forward(self, latent, out_1, out_3, out_4):\n",
    "    out = self.fc(latent)\n",
    "    out = out.view(out.size(0), 256, 3, 3)\n",
    "    #out = torch.cat([out, out_4], 1)\n",
    "    out = F.tanh(self.convTran1(out))\n",
    "    #out = torch.cat([out, out_3], 1)\n",
    "    out = F.tanh(self.convTran2(out))\n",
    "    out = F.tanh(self.convTran3(out))\n",
    "    #print(out.shape)\n",
    "    #print(out_1.shape)\n",
    "    out = torch.cat([out, out_1], 1)\n",
    "    #print(out.shape)\n",
    "    out = F.tanh(self.convTran4(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7d6059c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "   def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "   def forward(self, x):\n",
    "        latent, out_1, out_3, out_4 = self.encoder(x)\n",
    "        x_recon = self.decoder(latent, out_1, out_3, out_4)\n",
    "        return  x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5b76899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, train_loader_hd, val_loader, val_loader_hd, Epochs, loss_fn):\n",
    "    train_loss_avg = []\n",
    "    val_loss_avg = []\n",
    "    for epoch in range(Epochs):\n",
    "        train_loss_avg.append(0)\n",
    "        num_batches = 0\n",
    "        iterator = iter(train_loader_hd)\n",
    "        for image_batch, _ in train_loader:\n",
    "            image_batch = image_batch.to(device)\n",
    "            batch_list = next(iterator)\n",
    "            image_batch_hd = batch_list[0].to(device)\n",
    "            image_batch_recon = model(image_batch)\n",
    "            loss = loss_fn(image_batch_recon, image_batch_hd)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            train_loss_avg[-1] += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        train_loss_avg[-1] /= num_batches\n",
    "        print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, Epochs, train_loss_avg[-1]))\n",
    "        \n",
    "        iterator = iter(val_loader_hd)\n",
    "        val_loss_avg.append(0)\n",
    "        num_batches=0\n",
    "        for image_batch, _ in val_loader:\n",
    "            image_batch = image_batch.to(device)\n",
    "            batch_list = next(iterator)\n",
    "            image_batch_hd = batch_list[0].to(device)\n",
    "            image_batch_recon = model(image_batch)\n",
    "            with torch.no_grad():\n",
    "                loss = loss_fn(image_batch_recon, image_batch_hd)\n",
    "                val_loss_avg[-1] += loss.item()\n",
    "                num_batches += 1\n",
    "        val_loss_avg[-1] /= num_batches\n",
    "        print('Epoch [%d / %d] average reconstruction validation error: %f' % (epoch+1, Epochs, val_loss_avg[-1]))\n",
    "                \n",
    "    return train_loss_avg, val_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "eae7e996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 10] average reconstruction error: 0.264664\n",
      "Epoch [1 / 10] average reconstruction validation error: 0.222035\n",
      "Epoch [2 / 10] average reconstruction error: 0.189447\n",
      "Epoch [2 / 10] average reconstruction validation error: 0.145513\n",
      "Epoch [3 / 10] average reconstruction error: 0.125577\n",
      "Epoch [3 / 10] average reconstruction validation error: 0.104263\n",
      "Epoch [4 / 10] average reconstruction error: 0.098369\n",
      "Epoch [4 / 10] average reconstruction validation error: 0.090364\n",
      "Epoch [5 / 10] average reconstruction error: 0.106146\n",
      "Epoch [5 / 10] average reconstruction validation error: 0.090840\n",
      "Epoch [6 / 10] average reconstruction error: 0.087620\n",
      "Epoch [6 / 10] average reconstruction validation error: 0.084986\n",
      "Epoch [7 / 10] average reconstruction error: 0.084757\n",
      "Epoch [7 / 10] average reconstruction validation error: 0.083475\n",
      "Epoch [8 / 10] average reconstruction error: 0.084268\n",
      "Epoch [8 / 10] average reconstruction validation error: 0.084518\n",
      "Epoch [9 / 10] average reconstruction error: 0.083443\n",
      "Epoch [9 / 10] average reconstruction validation error: 0.082211\n",
      "Epoch [10 / 10] average reconstruction error: 0.083155\n",
      "Epoch [10 / 10] average reconstruction validation error: 0.082780\n"
     ]
    }
   ],
   "source": [
    "capacity = 64\n",
    "epochs = 10    \n",
    "learning_rate = 0.001\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(params=autoencoder.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "loss_result, loss_val = train(autoencoder,train_loader_low, train_loader_hd, val_loader_low, val_loader_hd, epochs, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, _ in train_loader_low:\n",
    "  image_batch = image_batch.to(device)\n",
    "  image_batch_recon = autoencoder(image_batch)\n",
    "  #Show_imgs(image_batch[0].cpu(),\"\")\n",
    "  #Show_imgs(image_batch_recon[0].cpu(),\"\")\n",
    "  #image_batch[0].cpu() \n",
    "  im = transform.ToPILImage()(image_batch[0]).convert(\"RGB\")  \n",
    "  display(im)\n",
    "  imt = transform.ToPILImage()(image_batch_recon[0]).convert(\"RGB\")\n",
    "  display(imt) \n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0f858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
